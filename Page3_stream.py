import pandas 
import streamlit as st
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
import seaborn as sns
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, RandomizedSearchCV
import plotly.express as px
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
import sklearn
import pandas as pd

# D√©finissez la largeur de la page Streamlit
st.set_page_config(layout="wide")

st.title("üöó La S√©curit√© Routi√®re √† V√©lo : Explorer et Pr√©venir ! üö¥‚Äç‚ôÇÔ∏è")
st.write("Bienvenue sur cette page d√©di√©e √† la s√©curit√© routi√®re, mettant l'accent sur les accidents impliquant des cyclistes. Nous plongeons dans les donn√©es routi√®res depuis 2005 pour comprendre, analyser et am√©liorer la s√©curit√© des cyclistes sur nos routes. Nous allons entra√Æner un mod√®le de machine learning afin d'√©tudier l'impact de diff√©rents facteur sur la gravit√© des accients de v√©lo.")
st.write("üìä Les Donn√©es en Bref : Notre exploration couvre une p√©riode √©tendue, nous permettant de saisir les √©volutions au fil des ann√©es. Ces donn√©es, issues de diff√©rentes sources, sont soigneusement analys√©es pour offrir des perspectives sur les accidents de la route impliquant des cyclistes.")
st.write("üåê Contexte R√©gional : √Ä travers notre analyse, nous explorerons les sp√©cificit√©s r√©gionales, car chaque r√©gion a ses propres caract√©ristiques et d√©fis en mati√®re de s√©curit√© routi√®re.")
st.write("üö® Objectif de cette page : Notre mission est de mieux comprendre les facteurs qui contribuent aux accidents de v√©lo, de pr√©voir les situations √† risque et d'optimiser les mod√®les de pr√©diction pour renforcer la s√©curit√© des cyclistes.")
st.write("Explorez avec nous les mod√®les, les tendances et les r√©sultats qui √©mergent de cette analyse approfondie. Ensemble, travaillons √† rendre nos routes plus s√ªres pour tous !")
st.write("Note : Nous concentrons notre analyse sur les ann√©es √† partir de 2015, offrant ainsi une vision actuelle des enjeux de s√©curit√© routi√®re √† v√©lo.")

# Utilisez pd.read_excel() pour lire le fichier Excel dans un DataFrame
@st.cache_data
def import_data():
    data = pandas.read_csv(r"Base_VO.csv", decimal=".", sep=",")  
    data = data.loc[data['an'] >= 2015]
    # Cr√©er une correspondance d√©partement - r√©gion
    departement_region = {
        1: 'Auvergne-Rh√¥ne-Alpes',
        2: 'Hauts-de-France',
        3: 'Auvergne-Rh√¥ne-Alpes',
        4: 'Provence-Alpes-C√¥te d\'Azur',
        5: 'Provence-Alpes-C√¥te d\'Azur',
        6: 'Provence-Alpes-C√¥te d\'Azur',
        7: 'Auvergne-Rh√¥ne-Alpes',
        8: 'Grand Est',
        9: 'Occitanie',
        10: 'Grand Est',
        11: 'Occitanie',
        12: 'Occitanie',
        13: 'Provence-Alpes-C√¥te d\'Azur',
        14: 'Normandie',
        15: 'Auvergne-Rh√¥ne-Alpes',
        16: 'Nouvelle-Aquitaine',
        17: 'Nouvelle-Aquitaine',
        18: 'Centre-Val de Loire',
        19: 'Nouvelle-Aquitaine',
        21: 'Bourgogne-Franche-Comt√©',
        22: 'Bretagne',
        23: 'Nouvelle-Aquitaine',
        24: 'Nouvelle-Aquitaine',
        25: 'Bourgogne-Franche-Comt√©',
        26: 'Auvergne-Rh√¥ne-Alpes',
        27: 'Normandie',
        28: 'Centre-Val de Loire',
        29: 'Bretagne',
        '2A': 'Corse',
        '2B': 'Corse',
        30: 'Occitanie',
        31: 'Occitanie',
        32: 'Occitanie',
        33: 'Nouvelle-Aquitaine',
        34: 'Occitanie',
        35: 'Bretagne',
        36: 'Centre-Val de Loire',
        37: 'Centre-Val de Loire',
        38: 'Auvergne-Rh√¥ne-Alpes',
        39: 'Bourgogne-Franche-Comt√©',
        40: 'Nouvelle-Aquitaine',
        41: 'Centre-Val de Loire',
        42: 'Auvergne-Rh√¥ne-Alpes',
        43: 'Auvergne-Rh√¥ne-Alpes',
        44: 'Pays de la Loire',
        45: 'Centre-Val de Loire',
        46: 'Occitanie',
        47: 'Nouvelle-Aquitaine',
        48: 'Occitanie',
        49: 'Pays de la Loire',
        50: 'Normandie',
        51: 'Grand Est',
        52: 'Grand Est',
        53: 'Pays de la Loire',
        54: 'Grand Est',
        55: 'Grand Est',
        56: 'Bretagne',
        57: 'Grand Est',
        58: 'Bourgogne-Franche-Comt√©',
        59: 'Hauts-de-France',
        60: 'Hauts-de-France',
        61: 'Normandie',
        62: 'Hauts-de-France',
        63: 'Auvergne-Rh√¥ne-Alpes',
        64: 'Nouvelle-Aquitaine',
        65: 'Occitanie',
        66: 'Occitanie',
        67: 'Grand Est',
        68: 'Grand Est',
        69: 'Auvergne-Rh√¥ne-Alpes',
        70: 'Bourgogne-Franche-Comt√©',
        71: 'Bourgogne-Franche-Comt√©',
        72: 'Pays de la Loire',
        73: 'Auvergne-Rh√¥ne-Alpes',
        74: 'Auvergne-Rh√¥ne-Alpes',
        75: '√éle-de-France',
        76: 'Normandie',
        77: '√éle-de-France',
        78: '√éle-de-France',
        79: 'Nouvelle-Aquitaine',
        80: 'Hauts-de-France',
        81: 'Occitanie',
        82: 'Occitanie',
        83: 'Provence-Alpes-C√¥te d\'Azur',
        84: 'Provence-Alpes-C√¥te d\'Azur',
        85: 'Pays de la Loire',
        86: 'Nouvelle-Aquitaine',
        87: 'Nouvelle-Aquitaine',
        88: 'Grand Est',
        89: 'Bourgogne-Franche-Comt√©',
        90: 'Bourgogne-Franche-Comt√©',
        91: '√éle-de-France',
        92: '√éle-de-France',
        93: '√éle-de-France',
        94: '√éle-de-France',
        95: '√éle-de-France',
        971: 'Guadeloupe',
        972: 'Martinique',
        973: 'Guyane',
        974: 'La R√©union',
        976: 'Mayotte'
    }
    data['region'] = data['dep'].map(departement_region)
    data = data.dropna(subset=['region'])
    data = data.dropna(subset=['dep'])

    region_mapping = {
        'Auvergne-Rh√¥ne-Alpes': 1,
        'Hauts-de-France': 2,
        'Provence-Alpes-C√¥te d\'Azur': 3,
        'Grand Est': 4,
        'Occitanie': 5,
        'Normandie': 6,
        'Centre-Val de Loire': 7,
        'Bourgogne-Franche-Comt√©': 8,
        'Bretagne': 9,
        'Corse': 10,
        'Pays de la Loire': 11,
        'Nouvelle-Aquitaine': 12,
        '√éle-de-France': 13,
        'Guadeloupe': 14,
        'Martinique': 15,
        'Guyane': 16,
        'La R√©union': 17,
        'Mayotte': 18
    }
    # Appliquer la correspondance √† la colonne 'region' de votre DataFrame
    data['region2'] = data['region'].map(region_mapping)
    data['grav'] = 0*data['indemne'] + 2*data['blesse_hospi'] + 1*data['hospi_leger'] + 3*data['tue']
    data['equipement'] = data['equipement'].apply(lambda x: eval(x) if isinstance(x, str) else x)
    return data

data = import_data()

@st.cache_data
def model(X_train, y_train, X_test, y_test):
    @st.cache_data
    def test_model(X_train, X_test, y_train, y_test):
        models = {
            'Random Forest': RandomForestClassifier(),
            'KNN': KNeighborsClassifier(),
            'XGBoost': XGBClassifier(),
            'SVM': SVC()
        }
        accuracies = {}
        matrices = {}

        for model_name, model in models.items():
            model.fit(X_train, y_train.values.ravel())
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            accuracies[model_name] = accuracy
            conf_matrix = confusion_matrix(y_test, y_pred)
            matrices[model_name] = conf_matrix
        
        return accuracies, matrices

    @st.cache_data
    def calculate_variable_importance(X_train, y_train):
        xgb_model = XGBClassifier()
        xgb_model.fit(X_train, y_train.values.ravel())
        importances_df = pd.DataFrame(index=X_train.columns)
        importances_df['XGBoost'] = xgb_model.feature_importances_
        return importances_df

    # Calculate variable importance
    importances_df = calculate_variable_importance(X_train, y_train)
    # Calculate accuracies and confusion matrices
    accuracies, confusion_matrices = test_model(X_train, X_test, y_train, y_test)

    # Display confusion matrices
    st.write("Affichons les matrices de confusion des diff√©rents mod√®les.")
    st.write("Une matrice de confusion est un moyen de visualiser o√π notre mod√®le a bien fonctionn√© et o√π il a eu des difficult√©s. Cela nous aide √† comprendre comment un mod√®le se comporte dans diff√©rentes situations.")
    fig, axes = plt.subplots(1, 4, figsize=(18, 4))
    for ax, (model_name, model_conf_matrix) in zip(axes, confusion_matrices.items()):
        ax.set_title(f"Matrice de Confusion pour {model_name}")
        total_per_class = model_conf_matrix.sum(axis=1)
        conf_matrix_percentage = (model_conf_matrix.T / total_per_class).T * 100
        sns.heatmap(conf_matrix_percentage, annot=True, fmt='.2f', cmap='Blues', cbar=False, ax=ax)
        ax.set_xlabel('Valeurs Pr√©dites')
        ax.set_ylabel('Valeurs R√©elles')
    plt.tight_layout()
    st.pyplot(fig)
    st.write("Lors de l\'observation des matrices de confusion pour chaque mod√®le, il est √©vident que les classes 0 (Indemnes) et 3 (Tu√©s) posent un d√©fi de pr√©diction significatif. Les pourcentages associ√©s √† ces classes sont notablement bas, sugg√©rant que les mod√®les ont du mal √† les identifier correctement. Cette difficult√© peut √™tre attribu√©e √† une repr√©sentation limit√©e de ces classes dans l\'ensemble de donn√©es, ce qui rend la g√©n√©ralisation plus complexe.")
    st.write("En revanche, les classes 1 (Bless√©s L√©gers) et 2 (Bless√©s hospitalis√©s) b√©n√©ficient d'une pr√©diction plus pr√©cise, avec des pourcentages plus √©lev√©s dans les matrices de confusion. Il semble que la repr√©sentation de ces classes soit plus robuste, facilitant ainsi la t√¢che des mod√®les.")

    # Display model accuracies
    st.write("En vue de prendre une d√©cision quant au choix du mod√®le, il est n√©cessaire d'examiner les pr√©cisions de chacun d'entre eux.")
    df_accuracies = pd.DataFrame.from_dict(accuracies, orient='index', columns=['Accuracy']).reset_index()
    st.dataframe(df_accuracies)
    st.write("Nous optons pour le mod√®le XGBoost dans notre application en raison de sa pr√©cision sup√©rieure parmi les mod√®les test√©s. Afin de simplifier l'interpr√©tation et d'accro√Ætre la stabilit√© du mod√®le, nous allons regrouper certaines classes de gravit√©. Les classes 'Indemne' et 'Bless√© L√©ger' ont √©t√© fusionn√©es, de m√™me que les classes 'Bless√© Hospitalis√©' et 'Tu√©'. Nous explorerons √©galement l'importance de chaque variable dans le mod√®le s√©lectionn√©.")

    # Display variable importance
    st.write("Affichons l'importance des variables pour le mod√®le XGBoost.")
    fig1 = px.bar(x=importances_df.index, y=importances_df['XGBoost'], title='Importance des variables explicatives utilis√©es pour le mod√®le XGBoost')
    fig1.update_layout(
        plot_bgcolor="rgba(0,0,0,0)",
        xaxis=dict(showgrid=False, title='Caract√©ristiques'),
        yaxis=dict(showgrid=False, title='Importance')
    )
    st.plotly_chart(fig1, use_container_width=True, figsize=(10, 6))
    st.write("Nous notons que les variables 'Dep' (d√©partements fran√ßais) et 'Catr' (type de route) se d√©marquent comme les plus cruciales dans notre mod√®le XGBoost. Nous pr√©voyons de les maintenir pour la suite de l'analyse.")
    st.write("Nous avons choisi d'utiliser un mod√®le XGBoost, en raison de sa capacit√© √† traiter des ensembles de donn√©es complexes et √† fournir des pr√©dictions pr√©cises. Apr√®s l'entra√Ænement initial, nous allons optimis√© les param√®tres du mod√®le pour am√©liorer sa pr√©cision. Pour ce faire, nous allons tester plusieurs combinaisons d'hyperparam√®tres et conserver celle qui maximise la pr√©cision de notre mod√®le (accuracy)")


# Fonction pour fusionner les classes de gravit√©
def merge_gravity_classes(y):
    return y.replace({0: 0, 1: 0, 2: 1, 3: 1})
# Fonction pour diviser les donn√©es et fusionner les classes
def split_and_merge_data(data):
    X = data[['dep', 'catr', 'homme']]
    y = data['grav']
    y_merged = merge_gravity_classes(y)
    return train_test_split(X, y_merged, test_size=0.2, random_state=42, stratify=y_merged)
def split_and_merge_data2(data):
    X = data[["secuexist", "age", "dep", "lum", "atm", "catr", "trajet", "equipement", "homme"]]
    y = data['grav']
    return train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Fonction pour effectuer une recherche al√©atoire des hyperparam√®tres et retourner le mod√®le
def perform_random_search(X_train, y_train):
    param_dist = {
        'learning_rate': [0.01, 0.1, 0.2],
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7],
        'subsample': [0.8, 0.9, 1.0],
        'colsample_bytree': [0.8, 0.9, 1.0],
        'gamma': [0, 1, 2],
        'min_child_weight': [1, 2, 3],
        'lambda': [0, 1, 2],
        'alpha': [0, 1, 2]
    }
    xgb = XGBClassifier()
    random_search = RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=10, scoring='accuracy', cv=3, random_state=42)
    random_search.fit(X_train, y_train)
    best_params = random_search.best_params_
    best_model = random_search.best_estimator_
    return best_params, best_model

# Fonction pour afficher la matrice de confusion sous forme de heatmap
def display_confusion_matrix_heatmap(y_test, y_pred):
    conf_matrix = confusion_matrix(y_test, y_pred)
    total_per_class = conf_matrix.sum(axis=1)
    conf_matrix_percentage = (conf_matrix.T / total_per_class).T * 100
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(conf_matrix_percentage, annot=True, fmt='.2f', cmap='Blues', cbar=False, ax=ax)
    ax.set_xlabel('Valeurs Pr√©dites')
    ax.set_ylabel('Valeurs R√©elles')
    plt.title("Matrice de Confusion")
    st.pyplot(fig)

# Application des fonctions
    
X_train, X_test, y_train, y_test = split_and_merge_data(data)
X_train2, X_test2, y_train2, y_test2 = split_and_merge_data2(data)

X_train2_no_missing = X_train2.dropna()
y_train2_no_missing = y_train2.loc[X_train2_no_missing.index]
X_test2_no_missing = X_test2.dropna()
y_test2_no_missing = y_test2.loc[X_test2_no_missing.index]

best_params, best_model = perform_random_search(X_train, y_train)
y_pred = best_model.predict(X_test)
# Calcul et affichage de l'accuracy
accuracy = accuracy_score(y_test, y_pred)

# Liste des r√©gions fran√ßaises m√©tropolitaines
departements = {
    1: "Ain",
    2: "Aisne",
    3: "Allier",
    4: "Alpes-de-Haute-Provence",
    5: "Hautes-Alpes",
    6: "Alpes-Maritimes",
    7: "Ard√®che",
    8: "Ardennes",
    9: "Ari√®ge",
    10: "Aube",
    11: "Aude",
    12: "Aveyron",
    13: "Bouches-du-Rh√¥ne",
    14: "Calvados",
    15: "Cantal",
    16: "Charente",
    17: "Charente-Maritime",
    18: "Cher",
    19: "Corr√®ze",
    21: "C√¥te-d'Or",
    22: "C√¥tes-d'Armor",
    23: "Creuse",
    24: "Dordogne",
    25: "Doubs",
    26: "Dr√¥me",
    27: "Eure",
    28: "Eure-et-Loir",
    29: "Finist√®re",
    '2A': "Corse-du-Sud",
    '2B': "Haute-Corse",
    30: "Gard",
    31: "Haute-Garonne",
    32: "Gers",
    33: "Gironde",
    34: "H√©rault",
    35: "Ille-et-Vilaine",
    36: "Indre",
    37: "Indre-et-Loire",
    38: "Is√®re",
    39: "Jura",
    40: "Landes",
    41: "Loir-et-Cher",
    42: "Loire",
    43: "Haute-Loire",
    44: "Loire-Atlantique",
    45: "Loiret",
    46: "Lot",
    47: "Lot-et-Garonne",
    48: "Loz√®re",
    49: "Maine-et-Loire",
    50: "Manche",
    51: "Marne",
    52: "Haute-Marne",
    53: "Mayenne",
    54: "Meurthe-et-Moselle",
    55: "Meuse",
    56: "Morbihan",
    57: "Moselle",
    58: "Ni√®vre",
    59: "Nord",
    60: "Oise",
    61: "Orne",
    62: "Pas-de-Calais",
    63: "Puy-de-D√¥me",
    64: "Pyr√©n√©es-Atlantiques",
    65: "Hautes-Pyr√©n√©es",
    66: "Pyr√©n√©es-Orientales",
    67: "Bas-Rhin",
    68: "Haut-Rhin",
    69: "Rh√¥ne",
    70: "Haute-Sa√¥ne",
    71: "Sa√¥ne-et-Loire",
    72: "Sarthe",
    73: "Savoie",
    74: "Haute-Savoie",
    75: "Paris",
    76: "Seine-Maritime",
    77: "Seine-et-Marne",
    78: "Yvelines",
    79: "Deux-S√®vres",
    80: "Somme",
    81: "Tarn",
    82: "Tarn-et-Garonne",
    83: "Var",
    84: "Vaucluse",
    85: "Vend√©e",
    86: "Vienne",
    87: "Haute-Vienne",
    88: "Vosges",
    89: "Yonne",
    90: "Territoire de Belfort",
    91: "Essonne",
    92: "Hauts-de-Seine",
    93: "Seine-Saint-Denis",
    94: "Val-de-Marne",
    95: "Val-d'Oise",
    971: "Guadeloupe",
    972: "Martinique",
    973: "Guyane",
    974: "La R√©union",
    976: "Mayotte"
}
# Cr√©er une liste des noms de d√©partements
departement_names = list(departements.values())
# Cr√©er une liste d√©roulante dans Streamlit avec les noms des d√©partements
selected_departement_name = st.selectbox("S√©lectionnez un d√©partement", departement_names)
# Convertir le nom de d√©partement en num√©ro en utilisant la liste departements
def get_departement_num(selected_departement_name):
    return [key for key, value in departements.items() if value == selected_departement_name][0]
# Section pour la s√©lection du sexe et de l'√¢ge
st.header('S√©lectionnez votre genre:')
gender_options = {'1': 'Masculin', '0': 'F√©minin'}
selected_gender_name = st.radio('Choisissez le sexe:', list(gender_options.values()))
# Convertir le genre en num√©ro en utilisant le dictionnaire gender_options
selected_gender_num = [key for key, value in gender_options.items() if value == selected_gender_name][0]
# Liste des types de routes
st.header('S√©lectionnez le type de routes que vous fr√©quentez le plus:')
catr_choice = {
    "1": 'Autoroute',
    "2": 'Route nationale',
    "3": 'Route d√©partementale',
    "4": 'Voie communale',
    "5": 'Hors r√©seau public',
    "6": 'Parc de stationnement ouvert √† la circulation publique',
    "7": 'Route de m√©tropole urbaine',
    "9": 'Autre'
}
selected_catr_name = st.selectbox('Choisissez le type de route:', list(catr_choice.values()))
# Convertir le type de route en num√©ro en utilisant le dictionnaire catr_choice
selected_catr_num = [key for key, value in catr_choice.items() if value == selected_catr_name][0]
# Fonction pour obtenir les donn√©es d'entr√©e en fonction des s√©lections de l'utilisateur
def get_user_input(selected_departement_name, selected_gender_name, selected_catr_name):
    return pd.DataFrame({
        "dep": [get_departement_num(selected_departement_name)],
        "catr": [int(selected_catr_num)],
        "homme": [int(selected_gender_num)]
    })

# Fonction pour pr√©dire et afficher les r√©sultats
def predict_and_display_results(user_input):
    predicted_class = best_model.predict(user_input)[0]
    st.write("En fonction des s√©lections effectu√©es, le mod√®le de machine learning pr√©dit la classe :", predicted_class)
    # Affichez la classe pr√©dite
    st.write("En fonction des s√©lections effectu√©es, le mod√®le de machine learning pr√©dit la classe :", predicted_class)
    if predicted_class == 0 :
        st.write("En d'autres termes, selon vos caract√©ristiques, notre mod√®le de machine learning pr√©dit la classe 0. Cela signifie qu'en cas d'accident, il pr√©voit que vous resterez indemne ou vous serez bless√© l√©g√®rement. Restez prudent ! ")
    else :
        st.write("En d'autres termes, selon vos caract√©ristiques, notre mod√®le de machine learning pr√©dit la classe 1. Cela signifie qu'en cas d'accident, il pr√©voit que vous serez bless√© gravement ou vous perdrez la vie. Restez prudent ! ")

    st.write("En plus de cela, nous avons quelques autres statistiques selon les caract√©ristiques inscrites :")

@st.cache_data
def import_data2():
    data2 = pd.read_csv(r"Base_dep.csv", decimal=".", sep=",")  
    return data2
data2 = import_data2()
bouton_test = st.button("R√©sultat avec mes donn√©es personnelles")

if bouton_test:
    st.write('Vous avez choisi la r√©gion:', selected_departement_name)
    st.write('Vous avez choisi le sexe:', selected_gender_name)
    st.write('Vous avez choisi le type de route:', selected_catr_name)

    # Pr√©parez les donn√©es d'entr√©e en fonction des s√©lections de l'utilisateur
    user_input = get_user_input(selected_departement_name, selected_gender_name, selected_catr_name)
    # Faites une pr√©diction avec le mod√®le XGBoost et affichez les r√©sultats
    predict_and_display_results(user_input)
    # Filtrer les donn√©es en fonction des s√©lections de l'utilisateur
    filtered_data = data2[(data2['dep'] == int(get_departement_num(selected_departement_name))) & (data2['catr'] == int(selected_catr_num)) & (data2['homme'] == int(selected_gender_num))]

    if not filtered_data.empty:
        st.write('Dans le cas d\'un accident, vous avez', filtered_data['part_indemne'].iloc[0]*100, '% de chance de rester indemne.')
        st.write('Dans le cas d\'un accident, vous avez', filtered_data['part_blesse_leger'].iloc[0]*100, '% de chance d\'√™tre bless√© l√©g√®rement.')
        st.write('Dans le cas d\'un accident, vous avez', filtered_data['part_blesse_hospi'].iloc[0]*100, '% de chance d\'√™tre bless√© et hospitalis√©.')
        st.write('Dans le cas d\'un accident, vous avez', filtered_data['part_tue'].iloc[0]*100, '% de chance de perdre la vie.')
    else:
        st.write("Aucune donn√©e suppl√©mentaire, nous sommes d√©sol√©es :)")

# Ajoutez un espace dans la barre lat√©rale
st.sidebar.write("Et pour les curieux :")

# Bouton dans la barre lat√©rale
button_clicked = st.sidebar.button("D√©couvrez et comprenez notre mod√®le")

# Si le bouton est cliqu√©, affichez le contenu de la page sp√©cifique
if button_clicked:
    st.title("Comprenons ensemble le mod√®le utilis√© et l'optimisation de celui-ci !")
    st.header("Objectif du Mod√®le : ")
    st.write("Le mod√®le que nous avons d√©velopp√© a pour objectif de pr√©dire la gravit√© des accidents de v√©lo en se basant sur divers facteurs. La gravit√© des accidents est class√©e en quatre cat√©gories : indemne, bless√© l√©ger, bless√© hospitalis√©, et tu√©. Comprendre la gravit√© des accidents pass√©s peut nous aider √† identifier les principaux contributeurs aux incidents graves, ce qui √† son tour peut informer des mesures de s√©curit√© cibl√©es pour r√©duire les risques futurs sur nos routes.")
    st.header("Variables Utilis√©es : ")
    st.write(" - Securit√© Existante (secuexist) : Repr√©sente les √©quipements de s√©curit√© port√©s par les individus impliqu√©s dans l'accident (casque, ceinture, etc.).")
    st.write("- √Çge (age) : L'√¢ge des personnes impliqu√©es dans l'accident.")
    st.write("- R√©gion (region) : La r√©gion g√©ographique o√π l'accident s'est produit. Chaque r√©gion a ses propres caract√©ristiques et d√©fis en mati√®re de s√©curit√© routi√®re.")
    st.write("- Luminosit√© (lum) : Les conditions d'√©clairage au moment de l'accident.")
    st.write("- Atmosph√®re (atm) : Les conditions atmosph√©riques au moment de l'accident.")
    st.write("- Type de Route (catr) : La cat√©gorie de route o√π l'accident s'est produit.")
    st.write("- Type de Trajet (trajet) : Le type de trajet effectu√© par les individus (domicile-travail, domicile-√©cole, etc.).")
    st.write("- √âquipement (equipement) : Les √©quipements sp√©cifiques utilis√©s lors de l'accident.")
    st.write("- Homme (sexe) : Genre de l'individu accident√©.")
    st.header("Observons les r√©sultats des diff√©rents mod√®les test√©s (KNN, SVM, Random Forest et XGBoost)")
    # Display the content of models presentation
    model(X_train2_no_missing, y_train2_no_missing, X_test2_no_missing, y_test2_no_missing)
    st.write("Voici la meilleurs combinaison d'hyperparam√®tres :", best_params)
    st.write("Avec cette combinaison d'hyperparam√®tres, nous obtenons l'accuracy suivante : ")
    st.write(accuracy)
    st.write("Nous constatons que la pr√©cision de notre mod√®le XGBoost, une fois optimis√©e, a augment√© de 10 points par rapport √† la version non optimis√©e. Ainsi, nous atteignons d√©sormais une pr√©cision d'environ 73%, ce qui se traduit par la capacit√© du mod√®le √† correctement classer 73 individus sur 100.")
    st.write("Afin de confirmer notre choix, int√©ressons nous √† la matrice de confusion :")
    # Affichage de la matrice de confusion sous forme de heatmap
    display_confusion_matrix_heatmap(y_test, y_pred)
    st.write("Le choix de notre mod√®le s'est av√©r√© judicieux, comme le d√©montre la matrice de confusion. Cette derni√®re offre une visualisation claire de la performance du mod√®le en mettant en √©vidence un grand nombre de pr√©dictions correctes et un nombre limit√© d'erreurs de pr√©diction. Les r√©sultats indiquent une capacit√© significative du mod√®le √† bien classifier les diff√©rentes classes de gravit√© des accidents de v√©lo. Nous observons une pr√©pond√©rance de pr√©dictions pr√©cises, illustrant ainsi la robustesse et la fiabilit√© de notre approche.")
        
